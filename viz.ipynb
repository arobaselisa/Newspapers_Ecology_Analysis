{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.manifold import TSNE \n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "model_rise = Word2Vec.load('model_rise')\n",
    "model_stable = Word2Vec.load('model_stable')\n",
    "model_peak = Word2Vec.load('model_peak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to align the models\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    # base_embed.init_sims(replace=True)\n",
    "    # other_embed.init_sims(replace=True)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "    \n",
    "    return other_embed\n",
    "\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "        \n",
    "        print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "        \n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copies of one model, model_rise, to avoid changes in the original during alignment\n",
    "model_w2v_new = Word2Vec(sg = 1, vector_size=120)\n",
    "model_w2v_new.build_vocab(set(model_rise.wv.index_to_key))\n",
    "\n",
    "model_w2v_new.wv.vectors = model_rise.wv.vectors\n",
    "\n",
    "model_w2v_new2 = Word2Vec(sg = 1, vector_size=120)\n",
    "model_w2v_new2.build_vocab(set(model_rise.wv.index_to_key))\n",
    "\n",
    "model_w2v_new2.wv.vectors = model_rise.wv.vectors\n",
    "\n",
    "model_w2v_new.reset_from(model_rise)\n",
    "model_w2v_new2.reset_from(model_rise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align model_peak and model_stable with the copies of model_rise\n",
    "model_peak_modified = smart_procrustes_align_gensim(model_w2v_new\n",
    "                                                    , model_peak\n",
    "                                                   )\n",
    "\n",
    "model_stable_modified = smart_procrustes_align_gensim(model_w2v_new2\n",
    "                                                    , model_stable\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tsne(values):\n",
    "    if not values:\n",
    "        return \n",
    "\n",
    "    mat = np.array(values)\n",
    "    model = TSNE(n_components=2, random_state=0, learning_rate=150, init='pca')\n",
    "    fitted = model.fit_transform(mat)\n",
    "\n",
    "    return fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_sims(models, word1):\n",
    "\n",
    "    lookups = {}\n",
    "    sims = {}\n",
    "    for year, embed in models.items():\n",
    "        for word, sim in embed.wv.most_similar(word1, topn=10):\n",
    "            ww = \"%s|%s\" % (word, year)\n",
    "            if sim > 0.3:            \n",
    "                lookups[ww] = embed.wv[word]\n",
    "                sims[ww] = sim\n",
    "\n",
    "    return lookups, sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(word1, words, fitted, cmap, sims):\n",
    "    # TODO: remove this and just set the plot axes directly\n",
    "    plt.scatter(fitted[:,0], fitted[:,1], alpha=0)\n",
    "    plt.suptitle(\"%s\" % word1, fontsize=30, y=0.1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    annotations = []\n",
    "    colors = {1970:0.2, 1990:0.5,2000:0.7}\n",
    "    for i in range(len(words)):\n",
    "        pt = fitted[i]\n",
    "\n",
    "        ww,decade = [w.strip() for w in words[i].split(\"|\")]\n",
    "\n",
    "        if ww == word1:\n",
    "            annotations.append((ww, decade, pt))\n",
    "            word = word1+\" (\"+decade+\")\"\n",
    "            color = 'black'\n",
    "            sizing = 20\n",
    "            print(ww + \": annotations\")\n",
    "        else:\n",
    "            # word1 is the word we are plotting against\n",
    "            #color = cmap((int(decade)-1960)/100 + int(decade)/10000)\n",
    "            color = cmap(colors[int(decade)])\n",
    "            word = ww\n",
    "            sizing = sims[words[i]] * 30\n",
    "\n",
    "        plt.text(pt[0], pt[1], word, size=int(sizing), color=color)\n",
    "\n",
    "    return annotations        \n",
    "\n",
    "def plot_annotations(annotations):\n",
    "    # draw the movement between the word through the decades as a series of\n",
    "    # annotations on the graph\n",
    "    annotations.sort(key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    prev = annotations[0][-1]\n",
    "    for ww, decade, ann in annotations[1:]:\n",
    "        plt.annotate('', xy=prev, xytext=ann,\n",
    "            arrowprops=dict(facecolor='grey', shrink=0.1, alpha=0.3,width=2, headwidth=10))\n",
    "        prev = ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot of the semantic changes of ecologie for the three epochs\n",
    "models = {1970: model_peak_modified, 1990:model_rise, 2000:model_stable_modified}\n",
    "word1 = \"Ã©cologie\"\n",
    "\n",
    "lookups, sims = get_time_sims(models, word1)\n",
    "\n",
    "for decade, model in models.items():\n",
    "    ww = \"%s|%s\" % (word1, decade)\n",
    "    lookups[ww]=model.wv[word1]\n",
    "    \n",
    "words = list(lookups.keys())\n",
    "values = [lookups[word] for word in words ]\n",
    "fitted = fit_tsne(values)\n",
    "\n",
    "cmap = plt.cm.get_cmap(\"jet\", len(sims))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "annotations = plot_words(word1, words, fitted, cmap, sims)\n",
    "if annotations:\n",
    "    plot_annotations(annotations)\n",
    "\n",
    "plt.savefig(\"%s_chain.png\" % word1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

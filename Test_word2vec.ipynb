{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import dateparser\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmm/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "\n",
    "## Load the data\n",
    "First, let's load the data into a DataFrame, remove duplicates if there are any and drop rows with articles not recognized as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles_écologiquement.csv  :  815  | # NaNs :  3\n",
      "articles_écologismes.csv  :  2  | # NaNs :  0\n",
      "articles_écologistes.csv  :  11000  | # NaNs :  22\n",
      "articles_écologie.csv  :  2810  | # NaNs :  5\n",
      "articles_écologies.csv  :  8  | # NaNs :  0\n",
      "articles_écologisme.csv  :  43  | # NaNs :  0\n",
      "articles_écologiste.csv  :  7100  | # NaNs :  7\n",
      "CPU times: user 38 s, sys: 2.42 s, total: 40.4 s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# check how long does it take to run the cell\n",
    "\n",
    "DATA_PATH = \"../Data/\"\n",
    "\n",
    "articles = pd.DataFrame()\n",
    "# read the csvs file by file\n",
    "for f in listdir(DATA_PATH):\n",
    "    curr_df = pd.read_csv(DATA_PATH+f\n",
    "                          , usecols = ['Article Title', 'Journal', 'Date', 'Url', 'Text']\n",
    "                          , parse_dates=['Date']\n",
    "                          , date_parser=dateparser.parse\n",
    "                         )\n",
    "    # add the word used to retrieve the article\n",
    "    curr_df['word'] = f.split('.')[0].split('_')[1]\n",
    "    articles = articles.append(curr_df)\n",
    "    print(f, ' : ', len(curr_df)\n",
    "          , ' | # NaNs : ', np.count_nonzero(curr_df.isnull().values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates in the DF.\n"
     ]
    }
   ],
   "source": [
    "# check that all articles are different \n",
    "if not articles.Url.is_unique:\n",
    "    print(\"Removing duplicates\")\n",
    "    articles.drop_duplicates(subset=\"Url\"\n",
    "                             , keep='first'\n",
    "                             , inplace=True\n",
    "                             , ignore_index=False\n",
    "                            )\n",
    "else:\n",
    "    print(\"No duplicates in the DF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 articles not recognized as strings.\n"
     ]
    }
   ],
   "source": [
    "string_mask = [type(text)==str for text in articles[\"Text\"]]\n",
    "print(\"There are {0} articles not recognized as strings.\"\\\n",
    "      .format(len(string_mask)-np.sum(string_mask)))\n",
    "# remove articles that aren't string\n",
    "articles = articles[string_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Date</th>\n",
       "      <th>Url</th>\n",
       "      <th>Text</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>8920</td>\n",
       "      <td>pnr.iR RI nr.n</td>\n",
       "      <td>La Liberté</td>\n",
       "      <td>2005-11-14</td>\n",
       "      <td>https://www.e-newspaperarchives.ch/?a=d&amp;d=LLE2...</td>\n",
       "      <td>pnr . iR RI nr . n  dats de gauche , les deux...</td>\n",
       "      <td>écologistes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8027</th>\n",
       "      <td>7220</td>\n",
       "      <td>Sept organisations critiquent le manque ...</td>\n",
       "      <td>La Liberté</td>\n",
       "      <td>1994-02-19</td>\n",
       "      <td>https://www.e-newspaperarchives.ch/?a=d&amp;d=LLE1...</td>\n",
       "      <td>Sept organisations critiquent le manque de vo...</td>\n",
       "      <td>écologistes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>9196</td>\n",
       "      <td>Puisse Mitterrand sauvegarder la liberté...</td>\n",
       "      <td>La Gazette</td>\n",
       "      <td>1981-05-28</td>\n",
       "      <td>https://www.e-newspaperarchives.ch/?a=d&amp;d=GDM1...</td>\n",
       "      <td>Puisse Mitterrand sauvegarder la liberté !  U...</td>\n",
       "      <td>écologistes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                 Article Title     Journal  \\\n",
       "9727    8920                               pnr.iR RI nr.n   La Liberté   \n",
       "8027    7220  Sept organisations critiquent le manque ...   La Liberté   \n",
       "10003   9196  Puisse Mitterrand sauvegarder la liberté...   La Gazette   \n",
       "\n",
       "            Date                                                Url  \\\n",
       "9727  2005-11-14  https://www.e-newspaperarchives.ch/?a=d&d=LLE2...   \n",
       "8027  1994-02-19  https://www.e-newspaperarchives.ch/?a=d&d=LLE1...   \n",
       "10003 1981-05-28  https://www.e-newspaperarchives.ch/?a=d&d=GDM1...   \n",
       "\n",
       "                                                    Text         word  \n",
       "9727    pnr . iR RI nr . n  dats de gauche , les deux...  écologistes  \n",
       "8027    Sept organisations critiquent le manque de vo...  écologistes  \n",
       "10003   Puisse Mitterrand sauvegarder la liberté !  U...  écologistes  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.reset_index(inplace=True)\n",
    "articles.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data concierge\n",
    "\n",
    "Now let's do some basic textual data cleaning by removing:\n",
    "- stopwords from nltk's french list, augmented manually\n",
    "- punctuation\n",
    "- numerical values\n",
    "- isolated letters\n",
    "\n",
    "To do so, we need to tokenize the textual strings first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_FR = stopwords.words(\"french\")\n",
    "STOP_FR += [\"comme\", \"tout\", \"aussi\", \"sans\", \"si\", \"selon\"]\n",
    "STOP_FR += [\"de\", \"du\", \"des\"] # added bc of tfidf on epochs\n",
    "\n",
    "PUNCTUATION = [punc for punc in string.punctuation]\n",
    "PUNCTUATION += [\"›\", \"__\", \"’\", \"‘\", \"...\", \"„\", \"¦\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_list_articles(list_articles):\n",
    "    \"\"\" tokenize iterable object and remove some pre-defined values \"\"\"\n",
    "    tokenized_articles = [word_tokenize(article) for article in list_articles]\n",
    "    tokenized_articles = [[token.lower() for token in article \n",
    "                           if not (token.lower() in STOP_FR+PUNCTUATION\n",
    "                           or token.isnumeric()\n",
    "                           or len(token) < 2)\n",
    "                          ]\n",
    "                         for article in tokenized_articles]\n",
    "    return tokenized_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 697 ms, total: 1min 32s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "texts_articles = list(articles[\"Text\"].values)\n",
    "tokenized_articles = tokenize_list_articles(texts_articles)\n",
    "# add a column in the DF:\n",
    "articles[\"tokens\"] = tokenized_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then quickly check the proportion of tokens collected that exist in the french vocabulary thanks to the `spellchecker` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the dataset: 6.17e+06.\n"
     ]
    }
   ],
   "source": [
    "# create a 1D list containing all the tokes\n",
    "all_tokens = [token \n",
    "              for article in tokenized_articles\n",
    "              for token in article\n",
    "             ]\n",
    "print(\"Number of tokens in the dataset: {0:.3g}.\".format(len(all_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.58% of the tokens are considered misspelled.\n"
     ]
    }
   ],
   "source": [
    "french_checker = SpellChecker(language='fr')  # use the french Dictionary\n",
    "\n",
    "misspelled_tokens = french_checker.unknown(all_tokens)\n",
    "print(\"{0:.3g}% of the tokens are considered misspelled.\".format(100*len(misspelled_tokens)/len(all_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an example of such misspelled words in a random article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In article 9203; 19 out of 324 words are considered incorrect. Those are: {'arth-goldau', 'quenr', 'raccordements', 'lfa', 'lotschberg', 'week-end', 'lallégement', 'romande', 'lécologie', 'votations', 'letat', 'lugano', 'gothard', 'nlfa', 'lintérieur', 'sz', 'sextupleront', 'redimensionné', 'neri'}\n"
     ]
    }
   ],
   "source": [
    "rand_index = np.random.randint(0, len(tokenized_articles))\n",
    "\n",
    "rand_tokens = tokenized_articles[rand_index]\n",
    "\n",
    "misspelled_rand = french_checker.unknown(rand_tokens)\n",
    "\n",
    "print(\"In article {0}; {1} out of {2} words are considered incorrect. Those are: {3}\"\\\n",
    "      .format(rand_index, len(misspelled_rand), len(rand_tokens), misspelled_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed the words in a word-space using Word2Vec\n",
    "\n",
    "## Naive approach\n",
    "To familiarize with the techniques and get a first glimpse on the possible outcomes, let's perform the word2vec embedding on the whole dataset.\n",
    "Note that, as defined below, the model is not deterministic, meaning that running it twice won't provide the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 49s, sys: 482 ms, total: 4min 49s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(sentences=tokenized_articles\n",
    "                       , vector_size=100\n",
    "                       , window=5\n",
    "                       , min_count=1\n",
    "                       , workers=4\n",
    "                       , sg=1 #skipgram\n",
    "                       , negative=5 #use of negative sampling\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is built, we can check what words are the closest to \"écologie\" in this wordspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('économie', 0.7515649795532227),\n",
       " ('ergonomie', 0.7334681153297424),\n",
       " ('rimer', 0.7291607856750488),\n",
       " ('ecologie', 0.7246688604354858),\n",
       " ('lécologie', 0.7239876985549927),\n",
       " ('réconcilier', 0.7174688577651978),\n",
       " ('larchitecture', 0.7072470188140869),\n",
       " ('spiritualité', 0.706907331943512),\n",
       " ('conjugue', 0.6987759470939636),\n",
       " ('féminisme', 0.6977463960647583),\n",
       " ('concilier', 0.695932924747467),\n",
       " ('appliquée', 0.6914093494415283),\n",
       " ('conflictuel', 0.6908039450645447),\n",
       " ('indissociables', 0.6903030276298523),\n",
       " ('consumérisme', 0.6873682737350464),\n",
       " ('concilie', 0.6866443753242493),\n",
       " ('antagonisme', 0.6827805638313293),\n",
       " ('equilibre', 0.6824730634689331),\n",
       " ('authenticité', 0.6817585229873657),\n",
       " ('réconcilie', 0.6809834837913513)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('embarqué', 0.8589066863059998),\n",
       " ('saoudienne', 0.8546707034111023),\n",
       " ('pereira', 0.8409197330474854),\n",
       " ('nemo', 0.8409045934677124),\n",
       " ('niger', 0.8390101194381714),\n",
       " ('pâtisserie', 0.8367184996604919),\n",
       " ('auschwitz', 0.8365958333015442),\n",
       " ('parrain', 0.8351282477378845),\n",
       " ('princesse', 0.8347744941711426),\n",
       " ('bete', 0.8336206078529358),\n",
       " ('ressuscite', 0.8324520587921143),\n",
       " ('arche', 0.8318435549736023),\n",
       " ('mickey', 0.8315181732177734),\n",
       " ('danseuse', 0.831042468547821),\n",
       " ('navy', 0.8307561874389648),\n",
       " ('nomade', 0.8305745124816895),\n",
       " ('traînée', 0.8302186131477356),\n",
       " ('bidonville', 0.8297882080078125),\n",
       " ('navigateur', 0.8297860622406006),\n",
       " ('constantinople', 0.8295069336891174)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore through different terms to get some insights and check that it makes sense\n",
    "model.wv.most_similar('noé', topn=20) # :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be verified that in these lists of words, the first value is a close word in the wordspace and the second value is the cosine similarity between those two terms. The following code allows also to play with the words to see how \"close\" or \"far\" two different words are in the built space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(word_vec1, word_vec2):\n",
    "    \"\"\" Compute the cosine similarity between two vectors in the wordspace \"\"\"\n",
    "    # if the string is provided, convert into vector thanks to the model\n",
    "    if type(word_vec1)==str:\n",
    "        word_vec1 = model.wv[word_vec1]\n",
    "    if type(word_vec2)==str:\n",
    "        word_vec2 = model.wv[word_vec2]\n",
    "        \n",
    "    return 1 - spatial.distance.cosine(word_vec1, word_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_dist(écologie, leitmotiv) = 0.667470395565033\n"
     ]
    }
   ],
   "source": [
    "vector_écologie = model.wv['écologie']  # get numpy vector of a word\n",
    "vector_leitmotiv = model.wv['leitmotiv']\n",
    "print(\"cosine_dist(écologie, leitmotiv) = {}\".format(cosine_sim(vector_écologie, vector_leitmotiv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add minimum count\n",
    "\n",
    "Still naive, but a bit less this time, let's build a model considering only words appearing a minimum of $N$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 44s, sys: 1.12 s, total: 3min 45s\n",
      "Wall time: 58.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N_min = 5\n",
    "\n",
    "model_min5 = Word2Vec(sentences=tokenized_articles\n",
    "                 , vector_size=100\n",
    "                 , window=5\n",
    "                 , min_count=N_min #minimum number of occurences of a word\n",
    "                 , workers=4\n",
    "                 , sg=1 #skipgram\n",
    "                 , negative=5 #use of negative sampling\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('économie', 0.7370924353599548),\n",
       " ('ecologie', 0.6811469793319702),\n",
       " ('ergonomie', 0.6783067584037781),\n",
       " ('rimer', 0.6582304239273071),\n",
       " ('réconcilier', 0.6573576927185059),\n",
       " ('spiritualité', 0.6463772654533386),\n",
       " ('conjugue', 0.6397854089736938),\n",
       " ('larchitecture', 0.6364557147026062),\n",
       " ('lécologie', 0.6356692910194397),\n",
       " ('florissante', 0.6310573816299438),\n",
       " ('dharmonie', 0.6273236274719238),\n",
       " (\"l'écologie\", 0.6226067543029785),\n",
       " ('conflictuel', 0.6217504143714905),\n",
       " ('logie', 0.62142413854599),\n",
       " ('concilier', 0.6203720569610596),\n",
       " ('equilibre', 0.6182593107223511),\n",
       " ('notions', 0.6142987608909607),\n",
       " ('indissociables', 0.6090261340141296),\n",
       " ('authenticité', 0.6062590479850769),\n",
       " ('economie', 0.6061153411865234)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_min5.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suissi', 0.7969109416007996),\n",
       " ('helvétique', 0.7586506009101868),\n",
       " ('romande', 0.7375638484954834),\n",
       " ('suis-', 0.72492915391922),\n",
       " ('ofel', 0.7181031703948975),\n",
       " ('usj', 0.711347758769989),\n",
       " ('suisses', 0.7095322012901306),\n",
       " ('asb', 0.7086243033409119),\n",
       " ('ucap', 0.703929603099823),\n",
       " ('lassociatior', 0.7009690999984741),\n",
       " ('feps', 0.6998518705368042),\n",
       " ('bos', 0.6981223821640015),\n",
       " ('cassure', 0.6974745392799377),\n",
       " ('radiodiffusion', 0.6974092721939087),\n",
       " ('horlogerie', 0.6956583857536316),\n",
       " ('sit', 0.6931045055389404),\n",
       " ('lasso', 0.6919788122177124),\n",
       " ('encourageait', 0.6905348896980286),\n",
       " ('rohner', 0.6900718808174133),\n",
       " ('horlogère', 0.6896616816520691)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('suisse', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build different models for different epochs \n",
    "\n",
    "To check if we can see some differences between the different time periods, let's split the data in 3 parts: prior to 1990, between 1990 and 2000 and after 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6860 6947 6417\n"
     ]
    }
   ],
   "source": [
    "mask_rise = [(date.year < 1990 and date.year > 1970) for date in pd.to_datetime(articles.Date)]\n",
    "mask_peak = [(date.year > 1990 and date.year < 2000) for date in pd.to_datetime(articles.Date)]\n",
    "mask_stable = [(date.year > 2000) for date in pd.to_datetime(articles.Date)]\n",
    "\n",
    "df_rise = articles[mask_rise]\n",
    "df_peak = articles[mask_peak]\n",
    "df_stable = articles[mask_stable]\n",
    "\n",
    "print(len(df_rise), len(df_peak), len(df_stable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 14s, sys: 795 ms, total: 4min 15s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model_rise = Word2Vec(sentences=df_rise.tokens.values\n",
    "                 , vector_size=100\n",
    "                 , window=5\n",
    "                 , min_count=1\n",
    "                 , workers=4\n",
    "                 , sg=1 #skipgram\n",
    "                 , negative=5 #use of negative sampling\n",
    "                )\n",
    "\n",
    "model_peak = Word2Vec(sentences=df_peak.tokens.values\n",
    "                 , vector_size=100\n",
    "                 , window=5\n",
    "                 , min_count=1\n",
    "                 , workers=4\n",
    "                 , sg=1 #skipgram\n",
    "                 , negative=5 #use of negative sampling\n",
    "                )\n",
    "\n",
    "model_stable = Word2Vec(sentences=df_stable.tokens.values\n",
    "                 , vector_size=100\n",
    "                 , window=5\n",
    "                 , min_count=1\n",
    "                 , workers=4\n",
    "                 , sg=1 #skipgram\n",
    "                 , negative=5 #use of negative sampling\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('economie', 0.7366136908531189),\n",
       " ('ecologie', 0.7022604942321777),\n",
       " ('lécologie', 0.7017691135406494),\n",
       " ('féminisme', 0.6722659468650818),\n",
       " ('économie', 0.6662540435791016),\n",
       " ('science', 0.6643683314323425),\n",
       " ('sexuelle', 0.6618438959121704),\n",
       " ('biologie', 0.6524724364280701),\n",
       " ('solidarité', 0.6521098017692566),\n",
       " ('psychologie', 0.6428419947624207),\n",
       " ('éco', 0.6400136947631836),\n",
       " ('technocratie', 0.639086127281189),\n",
       " ('support', 0.6363763213157654),\n",
       " ('piliers', 0.6354470252990723),\n",
       " ('antagonisme', 0.6345431208610535),\n",
       " ('notions', 0.6318321228027344),\n",
       " ('léthique', 0.6314420104026794),\n",
       " ('réconcilier', 0.6301258206367493),\n",
       " ('éducation', 0.629490077495575),\n",
       " ('abordés', 0.628057062625885)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rise.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('économie', 0.7324155569076538),\n",
       " ('authenticité', 0.6863291263580322),\n",
       " ('culture', 0.6704477667808533),\n",
       " ('concilier', 0.6586199998855591),\n",
       " ('modernité', 0.645703136920929),\n",
       " ('economie', 0.6381278038024902),\n",
       " ('equilibre', 0.6337856650352478),\n",
       " ('ecologie', 0.6258373260498047),\n",
       " ('lécologie', 0.6194453239440918),\n",
       " ('adéquation', 0.6145240068435669),\n",
       " ('comprise', 0.6121807098388672),\n",
       " ('rimer', 0.6091871857643127),\n",
       " ('conjuguer', 0.6075676083564758),\n",
       " ('sobriété', 0.6007258892059326),\n",
       " ('aérodynamisme', 0.6000484228134155),\n",
       " ('tendues', 0.597001314163208),\n",
       " ('réconcilier', 0.5963481068611145),\n",
       " ('parlez', 0.5935744047164917),\n",
       " ('et/ou', 0.5887662768363953),\n",
       " ('didées', 0.5880550742149353)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_peak.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('économie', 0.7390050888061523),\n",
       " ('architecture', 0.6903532147407532),\n",
       " ('ergonomie', 0.6667472720146179),\n",
       " ('durabilité', 0.664372980594635),\n",
       " ('spiritualité', 0.6639426946640015),\n",
       " ('science', 0.6580581068992615),\n",
       " ('antagonistes', 0.6546207070350647),\n",
       " ('lunité', 0.6513896584510803),\n",
       " ('réconcilier', 0.6509630680084229),\n",
       " ('concilier', 0.648421585559845),\n",
       " ('ecologie', 0.6478885412216187),\n",
       " ('logie', 0.6476572155952454),\n",
       " ('décologie', 0.6454635262489319),\n",
       " ('évolution', 0.6450504660606384),\n",
       " ('alliant', 0.6390377879142761),\n",
       " ('concilie', 0.6353011131286621),\n",
       " ('mêle', 0.6349608302116394),\n",
       " ('psychologie', 0.6344118714332581),\n",
       " ('allie', 0.6329975128173828),\n",
       " ('conflictuel', 0.6321721076965332)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stable.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970-1980: cosinedist(écologie, science)= 0.664 | cosinedist(écologie, politique)= 0.416\n",
      "1990-2000: cosinedist(écologie, science)= 0.539 | cosinedist(écologie, politique)= 0.348\n",
      "2000-...: cosinedist(écologie, science)= 0.658 | cosinedist(écologie, politique)= 0.283\n"
     ]
    }
   ],
   "source": [
    "vector_écologie_rise = model_rise.wv['écologie']\n",
    "vector_écologie_peak = model_peak.wv['écologie']\n",
    "vector_écologie_stable = model_stable.wv['écologie']\n",
    "\n",
    "vector_science_rise = model_rise.wv['science']\n",
    "vector_politique_rise = model_rise.wv['politique']\n",
    "\n",
    "vector_science_peak = model_peak.wv['science']\n",
    "vector_politique_peak = model_peak.wv['politique']\n",
    "\n",
    "vector_science_stable = model_stable.wv['science']\n",
    "vector_politique_stable = model_stable.wv['politique']\n",
    "\n",
    "print(\"1970-1980: cosinedist(écologie, science)= {0:.3f} | cosinedist(écologie, politique)= {1:.3f}\"\\\n",
    "     .format(cosine_sim(vector_écologie_rise, vector_science_rise)\n",
    "             , cosine_sim(vector_écologie_rise, vector_politique_rise)\n",
    "            )\n",
    "     )\n",
    "\n",
    "print(\"1990-2000: cosinedist(écologie, science)= {0:.3f} | cosinedist(écologie, politique)= {1:.3f}\"\\\n",
    "     .format(cosine_sim(vector_écologie_peak, vector_science_peak)\n",
    "             , cosine_sim(vector_écologie_peak, vector_politique_peak)\n",
    "            )\n",
    "     )\n",
    "\n",
    "print(\"2000-...: cosinedist(écologie, science)= {0:.3f} | cosinedist(écologie, politique)= {1:.3f}\"\\\n",
    "     .format(cosine_sim(vector_écologie_stable, vector_science_stable)\n",
    "             , cosine_sim(vector_écologie_stable, vector_politique_stable)\n",
    "            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "Find topics on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_LDA.filter_extremes(no_below=N_min)\n",
    "\n",
    "corpus = [dictionary_LDA.doc2bow(article) for article in tokenized_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.1 s, sys: 8.5 s, total: 1min 2s\n",
      "Wall time: 52.8 s\n"
     ]
    }
   ],
   "source": [
    "num_topics = 6\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.013*\"suisse\" + 0.010*\"conseil\" + 0.008*\"fédéral\" + 0.006*\"contre\" + 0.005*\"loi\" + 0.005*\"non\" + 0.004*\"politique\" + 0.004*\"national\" + 0.004*\"être\" + 0.004*\"initiative\"\n",
      "\n",
      "1: 0.008*\"hier\" + 0.006*\"contre\" + 0.005*\"deux\" + 0.005*\"suisse\" + 0.005*\"ats\" + 0.004*\"écologiste\" + 0.004*\"après\" + 0.004*\"ans\" + 0.004*\"pays\" + 0.004*\"gouvernement\"\n",
      "\n",
      "2: 0.008*\"projet\" + 0.005*\"valais\" + 0.005*\"wwf\" + 0.005*\"canton\" + 0.004*\"protection\" + 0.004*\"commune\" + 0.004*\"deux\" + 0.004*\"francs\" + 0.003*\"entre\" + 0.003*\"fait\"\n",
      "\n",
      "3: 0.018*\"parti\" + 0.012*\"conseil\" + 0.009*\"gauche\" + 0.009*\"deux\" + 0.007*\"socialiste\" + 0.007*\"verts\" + 0.006*\"partis\" + 0.006*\"droite\" + 0.006*\"pdc\" + 0.006*\"voix\"\n",
      "\n",
      "4: 0.004*\"suisse\" + 0.004*\"être\" + 0.003*\"environnement\" + 0.003*\"moins\" + 0.003*\"production\" + 0.003*\"eau\" + 0.003*\"déchets\" + 0.003*\"produits\" + 0.003*\"encore\" + 0.003*\"pays\"\n",
      "\n",
      "5: 0.006*\"bien\" + 0.004*\"fait\" + 0.004*\"monde\" + 0.004*\"faire\" + 0.004*\"tous\" + 0.004*\"où\" + 0.003*\"vie\" + 0.003*\"ans\" + 0.003*\"très\" + 0.003*\"temps\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21760, 244146)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_TFIDF = vectorizer.fit_transform(articles[\"Text\"].values)\n",
    "\n",
    "X_TFIDF.shape #len(articles = 21760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20452  1266 18915 ... 14494 14091  5564]]\n",
      "[[ 38049]\n",
      " [147481]\n",
      " [121550]\n",
      " ...\n",
      " [174336]\n",
      " [ 54724]\n",
      " [ 54724]]\n"
     ]
    }
   ],
   "source": [
    "print(X_TFIDF.argmax(axis=0))\n",
    "print(X_TFIDF.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chauffer'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[38049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Nous tenons à votre disposition non seulement le système de chauffage LOW NOx le plus respectueux de lenvironnement , mais encore notre brochure gratuite Chauffer  écologiquement  . Cet opuscule vous informera en détail au sujet de lOrdonnance sur la protection de l air 92 ( OPair 92 ) . Il suffit d envoyer le coupon ci-dessous . Sic Nom , prénom : Ruen : ^^^^ NPA / Lieu : ^^^ ¦ ^ m ^ m Prièred adressercecouponàELCOSystèmesd energieSA , Chauffer ^ ¦¦ B _____________^___ B ^¦____ F  écologiquement  , Maison Rouge 28 , 3960 Sierre . chauffage écolologique '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[\"Text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ¦ EU  RADIO CHABLAIS  M 23 M 1 ESSSSM 8 . 00 Journal canadien 14752769 8 . 45 6 . 45 Télétubbies 44313721 7 . 15 NulSilence ça pousse 62836382 9 . 05 Zig le part ailleurs 37084856 8 . 30 GoutZag café 24096634 10 . 15 Charmants tes deau sur pierres brûlantes voisins 68471837 12 . 05 100 % Ques- 43262585 10 . 00 Quand les éléphants tions 22601914 13 . 05 Mise au point meurent 95155634 10 . 55 La Taule 85808634 14 . 30 Charmants voisins 59939214 12 . 40 Nulle part ailleurs 24528127 16 . 00 Le Journal 39246276 80706382 13 . 45 Pur et dur 22925382 16 . 30 Mediterraneo 17092566 17 . 05 15 . 20 La fidélité 64073160 18 . 00 Pyramide 79381769 17 . 30 Questions Dieu , le diable et Bob 67121818 pour un champion 17096382 18 . 15 18 . 25 Nulle part ailleurs cinéma Charmants voisins 72627740 20 . 00 94512011 19 . 00 Nulle part ailleurs Journal suisse 89378011 21 . 05 Le 21474585 20 . 35 Mickey les yeux point 97324479 22 . 00 Le journal bleus 48432450 22 . 15 Lérotisme vu 31889924 22 . 15 Cible émouvante par ... 53228905 23 . 45 Lundi boxe 81806112 0 . 00 Journal belge 76950030 0 . 45 Football : Charlton-lps-95556888 1 . 05 Cible émouvante wich 13837246 2 . 35 Le genou de 70568975 2 . 30 Mediterraneo Claire 19079915 4 . 15 Brialy fait son 45743449 3 . 00 Infos 11666994 3 . 05 cinéma 98969642 5 . 10 Cendrillon Le Point 61775333 Rhapsodie 49634197 i g « w « wy _!_ M  LA PREMIÈRE 8 . 35 On en parle 9 . 30 Mordicus 11 . 06 Les dicodeurs 12 . 07 Chacun pour tous 12 . 11 Salut les p tits zèLe ournai ae miai tren  la musique 10 . 05 Nou- 11 . 00 Arc-en-ciel avec Steeve  moires de  veautés du disque 11 . 30 Méridienne 12 . 04 Nota Bene 13 . 30 A vue d esprit 13 . 45 Musique dabord 15 . 50 Concert : Orchestre de la Radio Hongroise 17 . 30 Info culture j . ,, . _ ., _ , : , , , dio Hongroise 17 . 30 Info culture i  Dres i ^ su - .. ( i _ i _ irjij . , , *&lt; 3 i _ iT te 13 . 00 Café des arts 13 . 30 Tom- _ ,, , , r -M . • «_ , «_ - i c-, • „ n , n 17 . 36 Feuilleton musical 18 . 06 bouctou , 52 jours 14 . 04 Ouvert JazzZ „„„ ] ntes musica | es pour cause d inventaire 15 . 04 2 QM Les horiz £ ns dus Aram C est curieux ... 17 . 09 Presque rien „ , h Khalchaturian i un Arménien sur presque tout 18 . 00 Forums che 2 , es Soviets 3 / 3 Ses œuvres 19 . 05 Trafic 20 . 04 20 heures au 22 . 30 Domaine parlé 23 . 00 Les conteur 21 . 04 Le meilleur des mémoires de la musique mondes 22 . 04 La ligne de cœur „ 22 . 30 Le journal de la nuit RHONE FM 5 . 00 Radio réveil 6 . 00 Sans-des-ESPACE 2 sus-dessous avec Florian 9 . 00 Les 8 . 30 Domaine parlé 9 . 06 Les mé- pieds dans le plat avec Joëlle  10 . 30 Jinny de mes rêves 35959176 10 . 55 Un grau de sable et deau 36387301 11 . 50 Lance et compte 68980924 12 . 40 Récré Kids 18535160 14 . 10 Joseph Balsamo 34764672 15 . 10 Lance et compte 15209189 16 . 00 Hill Street Blues 45655059 17 . 00 Orques de Norvège 92725059 17 . 30 Le National Géographie 92795818 18 . 00 Jinny 66433634 18 . 25 Une maman formidable 99493363 18 . 50 La panthère rose 49179189 19 . 00 Flash infos 78883924 19 . 25 Hill Street Blues 66502943 20 . 35 Pendant la pub 27936127 20 . 55 Mademoiselle Jaïre 77187189 23 . 00 Hercule Poirot 20770011 0 . 50 Pendant la pub 60197997 1 . 10 Jo- . seph Balsamo 17032420  13 . 00 Débrayages 16 . 00 Le Festival avec Sébastien 18 . 15 Les menteurs 19 . 00 Voix off avec Cynthia 20 . 00 Best of avec Patrick 21 . 00 Rencontre avec Yvan  5 . 30 Les Matinales 5 . 30 , 6 . 30 , 7 . 30 Flashs infos 6 . 00 , 7 . 00 , 8 . 00 Journal du matin 8 . 30 Magazine du matin 9 . 00 Contact 11 . 00 Infos 12 . 00 Infos . Le 12-13 13 . 00 L air de rien 17 . 00 Infos . Trajectoire : Frantz Weber ,  écologiste  VD 18 . 00 Le journal du soir . Le 18-19 19 . 00 Jazz 21 . 00 Le meilleur de la musique '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[\"Text\"].values[20452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rise = ' '.join(df_rise.Text.values)\n",
    "text_peak = ' '.join(df_peak.Text.values)\n",
    "text_stable = ' '.join(df_stable.Text.values)\n",
    "\n",
    "docs = [text_rise, text_peak, text_stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 231772)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_epochs = TfidfVectorizer()\n",
    "X_TFIDF_epochs = vectorizer_epochs.fit_transform(docs)\n",
    "\n",
    "X_TFIDF_epochs.shape #len(articles = 21760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51752]\n",
      " [51752]\n",
      " [51752]]\n"
     ]
    }
   ],
   "source": [
    "print(X_TFIDF_epochs.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_epochs.get_feature_names()[51752]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21760x244146 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5805849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOOOP_RUNNIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-7a9ced22f732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSTOOOP_RUNNIG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'STOOOP_RUNNIG' is not defined"
     ]
    }
   ],
   "source": [
    "STOOOP_RUNNIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pre_80.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_post_92.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to align models from different epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Code originally ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "# https://gist.github.com/tangert/106822a0f56f8308db3f1d77be2c7942\n",
    "def align_gensim_models(models, words=None):\n",
    "    \"\"\"\n",
    "    Returns the aligned/intersected models from a list of gensim word2vec models.\n",
    "    Generalized from original two-way intersection as seen above.\n",
    "    \n",
    "    Also updated to work with the most recent version of gensim\n",
    "    Requires reduce from functools\n",
    "    \n",
    "    In order to run this, make sure you run 'model.init_sims()' for each model before you input them for alignment.\n",
    "    \n",
    "    ##############################################\n",
    "    ORIGINAL DESCRIPTION\n",
    "    ##############################################\n",
    "    \n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocabs = [set(m.wv.key_to_index.keys()) for m in models]\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = reduce((lambda vocab1,vocab2: vocab1&vocab2), vocabs)\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    \n",
    "    # This was generalized from:\n",
    "    # if not vocab_m1-common_vocab and not vocab_m2-common_vocab and not vocab_m3-common_vocab:\n",
    "    #   return (m1,m2,m3)\n",
    "    if all(not vocab-common_vocab for vocab in vocabs):\n",
    "        print(\"All identical!\")\n",
    "        return models\n",
    "        \n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    #common_vocab.sort(key=lambda w: sum([m.wv.key_to_index[w] for m in models]),reverse=True)\n",
    "    \n",
    "    # Then for each model...\n",
    "    for m in models:\n",
    "        \n",
    "        # Replace old vectors_norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "                \n",
    "        old_arr = m.wv.get_normed_vectors()\n",
    "                \n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors_norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.wv.index2word = common_vocab\n",
    "        old_vocab = m.wv.key_to_index\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = Word2Vec.build_vocab(index=new_index, count=old_vocab_obj)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_gensim_models([model_pre_80, model_post_92], words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pre_80.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

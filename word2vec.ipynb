{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import dateparser\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed the words in a word-space using Word2Vec\n",
    "\n",
    "## Naive approach\n",
    "To familiarize with the techniques and get a first glimpse on the possible outcomes, let's perform the word2vec embedding on the whole dataset.\n",
    "Note that, as defined below, the model is not deterministic, meaning that running it twice won't provide the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into a Pandas DataFrame and parse the date\n",
    "articles_cor = pd.read_csv(\"./all_articles.csv\", parse_dates=['Date'], date_parser=dateparser.parse)\n",
    "# print 5 radom rows\n",
    "articles_cor.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_cor.tokens_cor = articles_cor.tokens_cor.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(sentences=articles_cor.tokens_cor.values\n",
    "                       , vector_size=100\n",
    "                       , window=5\n",
    "                       , min_count=1\n",
    "                       , workers=4\n",
    "                       , sg=1 #skipgram\n",
    "                       , negative=5 #use of negative sampling\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is built, we can check what words are the closest to \"écologie\" in this wordspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore through different terms to get some insights and check that it makes sense\n",
    "model.wv.most_similar('noé', topn=20) # :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be verified that in these lists of words, the first value is a close word in the wordspace and the second value is the cosine similarity between those two terms. The following code allows also to play with the words to see how \"close\" or \"far\" two different words are in the built space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(word_vec1, word_vec2):\n",
    "    \"\"\" Compute the cosine similarity between two vectors in the wordspace \"\"\"\n",
    "    # if the string is provided, convert into vector thanks to the model\n",
    "    if type(word_vec1)==str:\n",
    "        word_vec1 = model.wv[word_vec1]\n",
    "    if type(word_vec2)==str:\n",
    "        word_vec2 = model.wv[word_vec2]\n",
    "        \n",
    "    return 1 - spatial.distance.cosine(word_vec1, word_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_écologie = model.wv['écologie']  # get numpy vector of a word\n",
    "vector_leitmotiv = model.wv['noé']\n",
    "print(\"cosine_dist(écologie, noé) = {}\".format(cosine_sim(vector_écologie, vector_leitmotiv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build different models for different epochs \n",
    "\n",
    "To check if we can see some differences between the different time periods, let's split the data in 3 parts: prior to 1990, between 1990 and 2000 and after 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_rise = [(date.year < 1990 and date.year > 1970) for date in pd.to_datetime(articles_cor.Date)]\n",
    "mask_peak = [(date.year > 1990 and date.year < 2000) for date in pd.to_datetime(articles_cor.Date)]\n",
    "mask_stable = [(date.year > 2000) for date in pd.to_datetime(articles_cor.Date)]\n",
    "\n",
    "df_rise = articles_cor[mask_rise]\n",
    "df_peak = articles_cor[mask_peak]\n",
    "df_stable = articles_cor[mask_stable]\n",
    "\n",
    "print(len(df_rise), len(df_peak), len(df_stable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "model_rise = Word2Vec(sentences=df_rise.tokens_cor.values\n",
    "                 , vector_size=300\n",
    "                 , window=5\n",
    "                 , min_count=15\n",
    "                 , workers=4\n",
    "                 , sg=1 #skipgram\n",
    "                 , negative=6 #use of negative sampling\n",
    "                )\n",
    "\n",
    "model_peak = Word2Vec(sentences=df_peak.tokens_cor.values\n",
    "                 , vector_size=300\n",
    "                 , window=5\n",
    "                 , min_count=15\n",
    "                 , workers=4\n",
    "                 , sg=1 #skipgram\n",
    "                 , negative=6 #use of negative sampling\n",
    "                )\n",
    "\n",
    "model_stable = Word2Vec(sentences=df_stable.tokens_cor.values\n",
    "                 , vector_size=300\n",
    "                 , window=5\n",
    "                 , min_count=15\n",
    "                 , workers=4\n",
    "                 , sg=1 #skipgram\n",
    "                 , negative=6 #use of negative sampling\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rise.save('model_rise_cor_300')\n",
    "model_stable.save('model_stable_cor_300')\n",
    "model_peak.save('model_peak_cor_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rise.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_peak.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stable.wv.most_similar('écologie', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_écologie_rise = model_rise.wv['écologie']\n",
    "vector_écologie_peak = model_peak.wv['écologie']\n",
    "vector_écologie_stable = model_stable.wv['écologie']\n",
    "\n",
    "vector_science_rise = model_rise.wv['science']\n",
    "vector_politique_rise = model_rise.wv['politique']\n",
    "\n",
    "vector_science_peak = model_peak.wv['science']\n",
    "vector_politique_peak = model_peak.wv['politique']\n",
    "\n",
    "vector_science_stable = model_stable.wv['science']\n",
    "vector_politique_stable = model_stable.wv['politique']\n",
    "\n",
    "print(\"1970-1980: cosinedist(écologie, science)= {0:.3f} | cosinedist(écologie, politique)= {1:.3f}\"\\\n",
    "     .format(cosine_sim(vector_écologie_rise, vector_science_rise)\n",
    "             , cosine_sim(vector_écologie_rise, vector_politique_rise)\n",
    "            )\n",
    "     )\n",
    "\n",
    "print(\"1990-2000: cosinedist(écologie, science)= {0:.3f} | cosinedist(écologie, politique)= {1:.3f}\"\\\n",
    "     .format(cosine_sim(vector_écologie_peak, vector_science_peak)\n",
    "             , cosine_sim(vector_écologie_peak, vector_politique_peak)\n",
    "            )\n",
    "     )\n",
    "\n",
    "print(\"2000-...: cosinedist(écologie, science)= {0:.3f} | cosinedist(écologie, politique)= {1:.3f}\"\\\n",
    "     .format(cosine_sim(vector_écologie_stable, vector_science_stable)\n",
    "             , cosine_sim(vector_écologie_stable, vector_politique_stable)\n",
    "            )\n",
    "     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
